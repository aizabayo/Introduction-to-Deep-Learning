{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UbXkUQWFLBRF","trusted":true},"outputs":[],"source":["# HW2P2: Image Recognition and Verification"]},{"cell_type":"markdown","metadata":{"id":"VMg74_LaLL55"},"source":["This is the second homework  in 11785: Introduction to Deep Learning. We are trying to tackle the problem of Image Verification. For this, we will need to first train our own CNN model to tackle the problem of classification, consisting of 8631 identities. Using this, we get the face embeddings for different pairs of images and try to identify if the pair of face matches or not."]},{"cell_type":"markdown","metadata":{},"source":["### For this Assignment of Image Recognition and Verification, I tested different data transformation technique, model Architecture including ResNet, AlexNet, MobileFace, and Residual Squeeze Excite, and different optimizer incFor this assignment on image recognition and verification, I experimented with various data transformation techniques and model architectures, including ResNet, AlexNet, MobileFace, and Residual Squeeze-Excite networks. I also tested different optimizers such as Adam, AdamW, and SGD. Through numerous training runs tracked via WandB, my final experiment, which was 70 epochs, which returned better smaller Equal Error Rate (EER).luding Adam, AdamW,and SGD, I did many runs testing as shown by wandb and my last run was 70 epoch which gave me the good Equal Error Rate (EER) "]},{"cell_type":"markdown","metadata":{"id":"oaUgdhRqy8oT"},"source":[]},{"cell_type":"markdown","metadata":{"id":"695K5zs36a48"},"source":["# Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:55:13.534780Z","iopub.status.busy":"2024-10-11T11:55:13.534499Z","iopub.status.idle":"2024-10-11T11:55:14.635730Z","shell.execute_reply":"2024-10-11T11:55:14.634667Z","shell.execute_reply.started":"2024-10-11T11:55:13.534748Z"},"id":"sQr0ss8w6jVI","trusted":true},"outputs":[],"source":["!nvidia-smi # Run this to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:55:14.638171Z","iopub.status.busy":"2024-10-11T11:55:14.637822Z","iopub.status.idle":"2024-10-11T11:55:40.082970Z","shell.execute_reply":"2024-10-11T11:55:40.081579Z","shell.execute_reply.started":"2024-10-11T11:55:14.638130Z"},"id":"MmbTatic6PDX","trusted":true},"outputs":[],"source":["!pip install wandb --quiet # Install WandB\n","!pip install pytorch_metric_learning --quiet #Install the Pytorch Metric Library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:55:40.084763Z","iopub.status.busy":"2024-10-11T11:55:40.084430Z","iopub.status.idle":"2024-10-11T11:55:51.823006Z","shell.execute_reply":"2024-10-11T11:55:51.822080Z","shell.execute_reply.started":"2024-10-11T11:55:40.084727Z"},"trusted":true},"outputs":[],"source":["!pip install torchsummaryX==1.1.0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:55:51.825879Z","iopub.status.busy":"2024-10-11T11:55:51.825535Z","iopub.status.idle":"2024-10-11T11:55:59.543682Z","shell.execute_reply":"2024-10-11T11:55:59.542789Z","shell.execute_reply.started":"2024-10-11T11:55:51.825831Z"},"id":"_oCzGBTh6xjL","trusted":true},"outputs":[],"source":["import torch\n","from torchsummaryX import summary\n","import torchvision\n","from torchvision import transforms\n","import torch.nn.functional as F\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","from sklearn import metrics as mt\n","from scipy.optimize import brentq\n","from scipy.interpolate import interp1d\n","import glob\n","import wandb\n","import matplotlib.pyplot as plt\n","from pytorch_metric_learning import samplers\n","import csv\n","\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:55:59.545008Z","iopub.status.busy":"2024-10-11T11:55:59.544690Z","iopub.status.idle":"2024-10-11T11:55:59.549098Z","shell.execute_reply":"2024-10-11T11:55:59.548198Z","shell.execute_reply.started":"2024-10-11T11:55:59.544976Z"},"id":"MT9MZk9p69Q5","trusted":true},"outputs":[],"source":["# from google.colab import drive # Link to your drive if you are not using Colab with GCP\n","# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it here"]},{"cell_type":"markdown","metadata":{"id":"gf6Da1K37BSJ"},"source":["# Kaggle"]},{"cell_type":"markdown","metadata":{},"source":["### Setting up my kaggle account with my Api key"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T22:44:29.724972Z","iopub.status.busy":"2024-10-26T22:44:29.724676Z","iopub.status.idle":"2024-10-26T22:44:37.879434Z","shell.execute_reply":"2024-10-26T22:44:37.878309Z","shell.execute_reply.started":"2024-10-26T22:44:29.724939Z"},"id":"Z1Uu5z2K7AS3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting kaggle==1.5.8\n","  Downloading kaggle-1.5.8.tar.gz (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73249 sha256=abcd9a4b94b232ffe7aa39606a816d334da4f91a093ee99a478b4ebf2cd5df32\n","  Stored in directory: /root/.cache/pip/wheels/0b/76/ca/e58f8afa83166a0e68f0d5cd2e7f99d260bdc40e35da080eee\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.6.17\n","    Uninstalling kaggle-1.6.17:\n","      Successfully uninstalled kaggle-1.6.17\n","Successfully installed kaggle-1.5.8\n"]}],"source":["# TODO: Use the same Kaggle code from HW1P2\n","!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"angeizabayo\",\"key\":\"6727c97821f5c0aae21046c561c0545d\"}')\n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:56:06.024781Z","iopub.status.busy":"2024-10-11T11:56:06.024384Z","iopub.status.idle":"2024-10-11T11:57:29.685898Z","shell.execute_reply":"2024-10-11T11:57:29.684677Z","shell.execute_reply.started":"2024-10-11T11:56:06.024734Z"},"id":"sNRYbTmU7Dk3","trusted":true},"outputs":[],"source":["# # Reminder: Make sure you have connected your kaggle API before running this block\n","!mkdir '/kaggle/working'\n","\n","!kaggle competitions download -c 11785-hw-2-p-2-face-verification-fall-2024\n","!unzip -qo '11785-hw-2-p-2-face-verification-fall-2024.zip' -d '/kaggle/working'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:29.687698Z","iopub.status.busy":"2024-10-11T11:57:29.687358Z","iopub.status.idle":"2024-10-11T11:57:30.695233Z","shell.execute_reply":"2024-10-11T11:57:30.694271Z","shell.execute_reply.started":"2024-10-11T11:57:29.687662Z"},"trusted":true},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:30.696963Z","iopub.status.busy":"2024-10-11T11:57:30.696619Z","iopub.status.idle":"2024-10-11T11:57:31.695012Z","shell.execute_reply":"2024-10-11T11:57:31.693805Z","shell.execute_reply.started":"2024-10-11T11:57:30.696927Z"},"trusted":true},"outputs":[],"source":["!ls /kaggle/working"]},{"cell_type":"markdown","metadata":{"id":"9OgkfYwP7HVt"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:31.701164Z","iopub.status.busy":"2024-10-11T11:57:31.700564Z","iopub.status.idle":"2024-10-11T11:57:31.707738Z","shell.execute_reply":"2024-10-11T11:57:31.706875Z","shell.execute_reply.started":"2024-10-11T11:57:31.701100Z"},"id":"CMXkHmFc7G9m","trusted":true},"outputs":[],"source":["config = {\n","    'batch_size': 64, # Increase this if your GPU can handle it\n","    'lr': 0.001,\n","    'epochs': 70, # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically.\n","    'data_dir': \"/kaggle/working/11-785-f24-hw2p2-verification/cls_data\", #TODO\n","    'data_ver_dir': \"/kaggle/working/11-785-f24-hw2p2-verification/ver_data\", #TODO\n","    'pair_dir': \"/kaggle/working/11-785-f24-hw2p2-verification/\",\n","    'checkpoint_dir': \"/kaggle/working/kaggle/working/checkpoint\" #TODO\n","    # Include other parameters as needed.\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"EEAW65sB8Wlp"},"source":["# Dataset"]},{"cell_type":"markdown","metadata":{"id":"mPSk8DyK8htk"},"source":["## Dataset Class for doing Image Verification"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:31.709293Z","iopub.status.busy":"2024-10-11T11:57:31.708924Z","iopub.status.idle":"2024-10-11T11:57:31.724205Z","shell.execute_reply":"2024-10-11T11:57:31.723268Z","shell.execute_reply.started":"2024-10-11T11:57:31.709251Z"},"id":"KBleUieO8lwG","trusted":true},"outputs":[],"source":["class ImagePairDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, csv_file, transform):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.pairs = []\n","        if csv_file.endswith('.csv'):\n","            with open(csv_file, 'r') as f:\n","                reader = csv.reader(f)\n","                for i, row in enumerate(reader):\n","                    if i == 0:\n","                        continue\n","                    else:\n","                        self.pairs.append(row)\n","        else:\n","            with open(csv_file, 'r') as f:\n","                for line in f.readlines():\n","                    self.pairs.append(line.strip().split(' '))\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","\n","        img_path1, img_path2, match = self.pairs[idx]\n","        img1 = Image.open(os.path.join(self.data_dir, img_path1))\n","        img2 = Image.open(os.path.join(self.data_dir, img_path2))\n","        return self.transform(img1), self.transform(img2), int(match)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:31.726232Z","iopub.status.busy":"2024-10-11T11:57:31.725634Z","iopub.status.idle":"2024-10-11T11:57:31.736784Z","shell.execute_reply":"2024-10-11T11:57:31.735935Z","shell.execute_reply.started":"2024-10-11T11:57:31.726190Z"},"id":"xgBHYshwN9VY","trusted":true},"outputs":[],"source":["class TestImagePairDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, csv_file, transform):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.pairs = []\n","        if csv_file.endswith('.csv'):\n","            with open(csv_file, 'r') as f:\n","                reader = csv.reader(f)\n","                for i, row in enumerate(reader):\n","                    if i == 0:\n","                        continue\n","                    else:\n","                        self.pairs.append(row)\n","        else:\n","            with open(csv_file, 'r') as f:\n","                for line in f.readlines():\n","                    self.pairs.append(line.strip().split(' '))\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","\n","        img_path1, img_path2 = self.pairs[idx]\n","        img1 = Image.open(os.path.join(self.data_dir, img_path1))\n","        img2 = Image.open(os.path.join(self.data_dir, img_path2))\n","        return self.transform(img1), self.transform(img2)"]},{"cell_type":"markdown","metadata":{"id":"2j24TXNo9P97"},"source":["## Create Dataloaders for Image Recognition"]},{"cell_type":"markdown","metadata":{},"source":["### For Dataloader for image Recognition I modified training datasets  where I included data augmentation, Resizing of image, color Jitter, and kept normalizing as it was given. This transformation helped to enhance  the diversity of the training data and helping the model in learning.\n","\n","### Kept the  Validation transform and train and validation loader as they were given."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:31.738616Z","iopub.status.busy":"2024-10-11T11:57:31.738197Z","iopub.status.idle":"2024-10-11T11:57:37.693044Z","shell.execute_reply":"2024-10-11T11:57:37.691717Z","shell.execute_reply.started":"2024-10-11T11:57:31.738561Z"},"id":"taxotdUr9PfH","trusted":true},"outputs":[],"source":["data_dir = config['data_dir']\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir = os.path.join(data_dir, 'dev')\n","os.makedirs(config['checkpoint_dir'], exist_ok=True)\n","\n","# Train transforms\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(112),                     # Image Resizing \n","    torchvision.transforms.RandomPerspective(distortion_scale=0.3, p=0.5), \n","    torchvision.transforms.ColorJitter(brightness=0.2), \n","    torchvision.transforms.RandomRotation(degrees=25),   \n","    torchvision.transforms.RandomHorizontalFlip(p=0.25),     # Image Fliping\n","    torchvision.transforms.RandAugment(num_ops=4),          # Random Augmentation\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                     std=[0.5, 0.5, 0.5])  # Normalize the image\n","])\n","\n","# Validation transforms \n","val_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize(112),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5],\n","                                     std=[0.5, 0.5, 0.5])\n","])\n","\n","# Get Datasets\n","train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=train_transforms)\n","val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=val_transforms)\n","\n","# Data loaders\n","train_loader = torch.utils.data.DataLoader(train_dataset,\n","                                           batch_size=config[\"batch_size\"],\n","                                           shuffle=True,\n","                                           pin_memory=True,\n","                                           num_workers=8)\n","val_loader = torch.utils.data.DataLoader(val_dataset,\n","                                         batch_size=config[\"batch_size\"],\n","                                         shuffle=False,\n","                                         num_workers=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:37.695093Z","iopub.status.busy":"2024-10-11T11:57:37.694660Z","iopub.status.idle":"2024-10-11T11:57:37.717810Z","shell.execute_reply":"2024-10-11T11:57:37.716764Z","shell.execute_reply.started":"2024-10-11T11:57:37.695041Z"},"id":"YAsCf0Hq8pL9","trusted":true},"outputs":[],"source":["data_dir = config['data_ver_dir']\n","\n","pairs_dir = config['pair_dir']\n","# get datasets\n","val_dataset = os.path.join(config['data_ver_dir'], 'val_data')\n","\n","# TODO: Add your validation pair txt file\n","val_pairs = os.path.join(pairs_dir, 'val_pairs.txt') \n","pair_dataset = ImagePairDataset(data_dir, csv_file=val_pairs, transform=val_transforms)\n","pair_dataloader = torch.utils.data.DataLoader(pair_dataset,\n","                                              batch_size=config[\"batch_size\"],\n","                                              shuffle=False,\n","                                              pin_memory=True,\n","                                              num_workers=4)\n","test_data = os.path.join(config['data_ver_dir'], 'test_data')\n","# TODO: Add your validation pair txt file\n","test_pairs = os.path.join(pairs_dir, 'test_pairs.txt')\n","test_pair_dataset = TestImagePairDataset(data_dir, csv_file= test_pairs, transform=val_transforms)\n","test_pair_dataloader = torch.utils.data.DataLoader(test_pair_dataset,\n","                                              batch_size=config[\"batch_size\"],\n","                                              shuffle=False,\n","                                              pin_memory=True,\n","                                              num_workers=4)"]},{"cell_type":"markdown","metadata":{"id":"436KzM6u-3A2"},"source":["# EDA and Viz"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:37.720214Z","iopub.status.busy":"2024-10-11T11:57:37.719854Z","iopub.status.idle":"2024-10-11T11:57:37.897526Z","shell.execute_reply":"2024-10-11T11:57:37.896547Z","shell.execute_reply.started":"2024-10-11T11:57:37.720181Z"},"id":"AhnoHopx-0RB","trusted":true},"outputs":[],"source":["# Double-check your dataset/dataloaders work as expected\n","\n","print(\"Number of classes    : \", len(train_dataset.classes))\n","print(\"No. of train images  : \", train_dataset.__len__())\n","print(\"Shape of image       : \", train_dataset[0][0].shape)\n","print(\"Batch size           : \", config['batch_size'])\n","print(\"Train batches        : \", train_loader.__len__())\n","print(\"Val batches          : \", val_loader.__len__())\n","\n","# Feel free to print more things if needed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:37.899119Z","iopub.status.busy":"2024-10-11T11:57:37.898714Z","iopub.status.idle":"2024-10-11T11:57:43.063326Z","shell.execute_reply":"2024-10-11T11:57:43.062389Z","shell.execute_reply.started":"2024-10-11T11:57:37.899076Z"},"id":"CvKiA3GR_IPO","trusted":true},"outputs":[],"source":["# Visualize a few images in the dataset\n","\n","\"\"\"\n","You can write your own code, and you don't need to understand the code\n","It is highly recommended that you visualize your data augmentation as sanity check\n","\"\"\"\n","\n","r, c    = [5, 5]\n","fig, ax = plt.subplots(r, c, figsize= (15, 15))\n","\n","k       = 0\n","dtl     = torch.utils.data.DataLoader(\n","    dataset     = torchvision.datasets.ImageFolder(train_dir, transform= train_transforms), # dont wanna see the images with transforms\n","    batch_size  = config['batch_size'],\n","    shuffle     = True)\n","\n","for data in dtl:\n","    x, y = data\n","\n","    for i in range(r):\n","        for j in range(c):\n","            img = x[k].numpy().transpose(1, 2, 0)\n","            ax[i, j].imshow(img)\n","            ax[i, j].axis('off')\n","            k+=1\n","    break\n","\n","del dtl"]},{"cell_type":"markdown","metadata":{"id":"y3TUocDw_JU_"},"source":["# Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"yIR-dTHYyUov"},"source":["FAQ:\n","\n","**What's a very low early deadline architecture (mandatory early submission)**?\n","\n","- The very low early deadline architecture is a 5-layer CNN. Keep in mind the parameter limit is 18M.\n","- The first convolutional layer has 64 channels, kernel size 7, and stride 4. The next three have 128, 256, 512 and 1024 channels. Each have kernel size 3 and stride 2. Documentation to make convolutional layers: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","- Think about strided convolutions from the lecture, as convolutions with stride = 1 and downsampling. For strided convolution, what padding do you need for preserving the spatial resolution? (Hint => padding = kernel_size // 2) - Think why?\n","- Each convolutional layer is accompanied by a Batchnorm and ReLU layer.\n","- Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1. Use AdaptiveAvgPool2d. Documentation for AdaptiveAvgPool2d: https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html\n","- Then, remove (Flatten?) these trivial 1x1 dimensions away.\n","Look through https://pytorch.org/docs/stable/nn.html\n","\n","\n","**Why does a very simple network have 4 convolutions**?\n","\n","Input images are 112x112. Note that each of these convolutions downsample. Downsampling 2x effectively doubles the receptive field, increasing the spatial region each pixel extracts features from. Downsampling 32x is standard for most image models.\n","\n","**Why does a very simple network have high channel sizes**?\n","\n","Every time you downsample 2x, you do 4x less computation (at same channel size). To maintain the same level of computation, you 2x increase # of channels, which increases computation by 4x. So, balances out to same computation. Another intuition is - as you downsample, you lose spatial information. We want to preserve some of it in the channel dimension.\n","\n","**What is return_feats?**\n","\n","It essentially returns the second-to-last-layer features of a given image. It's a \"feature encoding\" of the input image, and you can use it for the verification task. You would use the outputs of the final classification layer for the classification task. You might also find that the classification outputs are sometimes better for verification too - try both."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:43.064818Z","iopub.status.busy":"2024-10-11T11:57:43.064518Z","iopub.status.idle":"2024-10-11T11:57:43.069375Z","shell.execute_reply":"2024-10-11T11:57:43.068504Z","shell.execute_reply.started":"2024-10-11T11:57:43.064786Z"},"trusted":true},"outputs":[],"source":["# # TODO: Fill out the model definition below\n","\n","# class Network(torch.nn.Module):\n","\n","#     def __init__(self, num_classes=8631):\n","#         super().__init__()\n","\n","#         self.backbone = torch.nn.Sequential(\n","#             # TODO\n","#             )\n","\n","#         self.cls_layer = #TODO\n","\n","#     def forward(self, x):\n","#             # TODO:\n","\n","#         return {\"feats\": feats, \"out\": out}\n","\n","# # Initialize your model\n","# model = Network().to(DEVICE)\n","# summary(model, (3, 112, 112))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:43.070919Z","iopub.status.busy":"2024-10-11T11:57:43.070573Z","iopub.status.idle":"2024-10-11T11:57:54.784742Z","shell.execute_reply":"2024-10-11T11:57:54.783591Z","shell.execute_reply.started":"2024-10-11T11:57:43.070875Z"},"trusted":true},"outputs":[],"source":["pip install torchsummary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchsummary import summary"]},{"cell_type":"markdown","metadata":{},"source":["### I expermented with different network and Residual Network Squeeze and Excite is the one yield the high cutoff of the homework. where others were performing very poorly in image recognitionn not even giving the low cutoff.\n","### - For the class of Squeeze and Excite I utilized global average pooling and and two fuly connected layer\n","### - for Residual Squeeze and Excite I employed two convolution and applyied batch normalization to prevent overfitting and improve generalization process, and utilized the ReLU as activation function.\n","### - Network class with convolution layer, batch Normalization,and ReLU as activation function, For effective feature extraction and stable training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:54.787238Z","iopub.status.busy":"2024-10-11T11:57:54.786888Z","iopub.status.idle":"2024-10-11T11:57:55.995108Z","shell.execute_reply":"2024-10-11T11:57:55.994197Z","shell.execute_reply.started":"2024-10-11T11:57:54.787201Z"},"trusted":true},"outputs":[],"source":["class SExcite(torch.nn.Module):\n","    def __init__(self, channels, reduction=16):\n","        super(SExcite, self).__init__()\n","        self.fc1 = torch.nn.Linear(channels, channels // reduction, bias=False)\n","        self.fc2 = torch.nn.Linear(channels // reduction, channels, bias=False)\n","    \n","    def forward(self, x):\n","        batch, channels, _, _ = x.size()\n","        y = x.view(batch, channels, -1).mean(dim=2)  # Global average pooling\n","        y = F.relu(self.fc1(y))\n","        y = torch.sigmoid(self.fc2(y))\n","        y = y.view(batch, channels, 1, 1)\n","        return x * y\n","\n","class ReSExcite(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None, reduction=16):\n","        super(ReSExcite, self).__init__()\n","        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n","        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n","        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n","        self.se = SExcite(out_channels, reduction)\n","        self.downsample = downsample\n","        self.relu = torch.nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.se(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        out += residual\n","        return self.relu(out)\n","\n","class Network(torch.nn.Module):\n","    def __init__(self, num_classes=8631):\n","        super(Network, self).__init__()\n","        self.backbone = torch.nn.Sequential(\n","            torch.nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n","            torch.nn.BatchNorm2d(64),\n","            torch.nn.ReLU(inplace=True),\n","            torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n","            self._make_layer(64, 64, 3),\n","            self._make_layer(64, 128, 4, stride=2),\n","            self._make_layer(128, 256, 6, stride=2),\n","            self._make_layer(256, 512, 3, stride=2),\n","            torch.nn.AdaptiveAvgPool2d((1, 1))\n","        )\n","        self.cls_layer = torch.nn.Linear(512, num_classes)\n","\n","    def _make_layer(self, in_channels, out_channels, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or in_channels != out_channels:\n","            downsample = torch.nn.Sequential(\n","                torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                torch.nn.BatchNorm2d(out_channels),\n","            )\n","\n","        layers = [ReSExcite(in_channels, out_channels, stride, downsample)]\n","        for _ in range(1, blocks):\n","            layers.append(ReSExcite(out_channels, out_channels))\n","        return torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        feats = self.backbone(x)\n","        feats = feats.view(feats.size(0), -1)\n","        out = self.cls_layer(feats)\n","        return {\"feats\": feats, \"out\": out}\n","\n","# Initialize the model\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = Network().to(DEVICE)\n","\n","# Summary of the model\n","summary(model, (3, 112, 112))\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["### For Criterion I used CrossEntropyLoss, and about optimizer I experiment on Adam, AdamW, and SGD is the one that gave the good EER compare to other optimizer with weight decay for regulerizing of 0.001. and for scheduler I used CossineAnnealingLR."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:55.996980Z","iopub.status.busy":"2024-10-11T11:57:55.996473Z","iopub.status.idle":"2024-10-11T11:57:56.006164Z","shell.execute_reply":"2024-10-11T11:57:56.005244Z","shell.execute_reply.started":"2024-10-11T11:57:55.996934Z"},"id":"pDP--pND_3Vy","trusted":true},"outputs":[],"source":["# --------------------------------------------------- #\n","\n","# Defining Loss function\n","#criterion = # TODO: What loss do you need for a multi class classification problem and would label smoothing be beneficial here?\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# --------------------------------------------------- #\n","\n","# Defining Optimizer\n","#optimizer =  # TODO: Feel free to pick a optimizer\n","\n","# --------------------------------------------------- #\n","import torch.optim as optim\n","\n","# Define the SGD optimizer\n","optimizer = optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-3       \n",")\n","\n","# Optionally, define a learning rate scheduler\n","\n","# Defining Scheduler\n","#scheduler = None# TODO: Use a good scheduler such as ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=60, eta_min=1e-6)\n","\n","# --------------------------------------------------- #\n","\n","# Initialising mixed-precision training. # Good news. We've already implemented FP16 (Mixed precision training) for you\n","# It is useful only in the case of compatible GPUs such as T4/V100\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"-d5ZDQfpw7gR"},"source":["# Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.007702Z","iopub.status.busy":"2024-10-11T11:57:56.007423Z","iopub.status.idle":"2024-10-11T11:57:56.020157Z","shell.execute_reply":"2024-10-11T11:57:56.019309Z","shell.execute_reply.started":"2024-10-11T11:57:56.007672Z"},"id":"7Ecg0J2sw9jJ","trusted":true},"outputs":[],"source":["class AverageMeter:\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.021929Z","iopub.status.busy":"2024-10-11T11:57:56.021306Z","iopub.status.idle":"2024-10-11T11:57:56.033008Z","shell.execute_reply":"2024-10-11T11:57:56.032206Z","shell.execute_reply.started":"2024-10-11T11:57:56.021886Z"},"id":"TqVw0ab0xBKT","trusted":true},"outputs":[],"source":["def accuracy(output, target, topk=(1,)):\n","    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n","    maxk = min(max(topk), output.size()[1])\n","    batch_size = target.size(0)\n","    _, pred = output.topk(maxk, 1, True, True)\n","    pred = pred.t()\n","    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n","    return [correct[:min(k, maxk)].reshape(-1).float().sum(0) * 100. / batch_size for k in topk]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.034358Z","iopub.status.busy":"2024-10-11T11:57:56.034097Z","iopub.status.idle":"2024-10-11T11:57:56.043924Z","shell.execute_reply":"2024-10-11T11:57:56.043177Z","shell.execute_reply.started":"2024-10-11T11:57:56.034324Z"},"id":"uNCQjz2RxD5S","trusted":true},"outputs":[],"source":["def get_ver_metrics(labels, scores, FPRs):\n","    # eer and auc\n","    fpr, tpr, _ = mt.roc_curve(labels, scores, pos_label=1)\n","    roc_curve = interp1d(fpr, tpr)\n","    EER = 100. * brentq(lambda x : 1. - x - roc_curve(x), 0., 1.)\n","    AUC = 100. * mt.auc(fpr, tpr)\n","\n","    # get acc\n","    tnr = 1. - fpr\n","    pos_num = labels.count(1)\n","    neg_num = labels.count(0)\n","    ACC = 100. * max(tpr * pos_num + tnr * neg_num) / len(labels)\n","\n","    # TPR @ FPR\n","    if isinstance(FPRs, list):\n","        TPRs = [\n","            ('TPR@FPR={}'.format(FPR), 100. * roc_curve(float(FPR)))\n","            for FPR in FPRs\n","        ]\n","    else:\n","        TPRs = []\n","\n","    return {\n","        'ACC': ACC,\n","        'EER': EER,\n","        'AUC': AUC,\n","        'TPRs': TPRs,\n","    }"]},{"cell_type":"markdown","metadata":{"id":"juUbZnP0AEUi"},"source":["# Train and Validation Function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.045584Z","iopub.status.busy":"2024-10-11T11:57:56.045279Z","iopub.status.idle":"2024-10-11T11:57:56.057026Z","shell.execute_reply":"2024-10-11T11:57:56.056118Z","shell.execute_reply.started":"2024-10-11T11:57:56.045544Z"},"id":"IMnxvQT-AHsu","trusted":true},"outputs":[],"source":["def train_epoch(model, dataloader, optimizer, lr_scheduler, scaler, device, config):\n","\n","    model.train()\n","\n","    # metric meters\n","    loss_m = AverageMeter()\n","    acc_m = AverageMeter()\n","\n","    # Progress Bar\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        # send to cuda\n","        images = images.to(device, non_blocking=True)\n","        if isinstance(labels, (tuple, list)):\n","            targets1, targets2, lam = labels\n","            labels = (targets1.to(device), targets2.to(device), lam)\n","        else:\n","            labels = labels.to(device, non_blocking=True)\n","\n","        # forward\n","        with torch.cuda.amp.autocast():  # This implements mixed precision. Thats it!\n","            outputs = model(images)\n","\n","            # Use the type of output depending on the loss function you want to use\n","            loss = criterion(outputs['out'], labels)\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","        # metrics\n","        loss_m.update(loss.item())\n","        if 'feats' in outputs:\n","            acc = accuracy(outputs['out'], labels)[0].item()\n","        else:\n","            acc = 0.0\n","        acc_m.update(acc)\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            # acc         = \"{:.04f}%\".format(100*accuracy),\n","            acc=\"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n","            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg),\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    # You may want to call some schedulers inside the train function. What are these?\n","    #if lr_scheduler is not None:\n","     #   lr_scheduler.step()\n","\n","    batch_bar.close()\n","\n","    return acc_m.avg, loss_m.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.058936Z","iopub.status.busy":"2024-10-11T11:57:56.058180Z","iopub.status.idle":"2024-10-11T11:57:56.071299Z","shell.execute_reply":"2024-10-11T11:57:56.070530Z","shell.execute_reply.started":"2024-10-11T11:57:56.058898Z"},"id":"5qkdH295wNUX","trusted":true},"outputs":[],"source":["@torch.no_grad()\n","def valid_epoch_cls(model, dataloader, device, config):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val Cls.', ncols=5)\n","\n","    # metric meters\n","    loss_m = AverageMeter()\n","    acc_m = AverageMeter()\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","            loss = criterion(outputs['out'], labels)\n","\n","        # metrics\n","        acc = accuracy(outputs['out'], labels)[0].item()\n","        loss_m.update(loss.item())\n","        acc_m.update(acc)\n","\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}% ({:.04f})\".format(acc, acc_m.avg),\n","            loss        = \"{:.04f} ({:.04f})\".format(loss.item(), loss_m.avg))\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    return acc_m.avg, loss_m.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.072606Z","iopub.status.busy":"2024-10-11T11:57:56.072326Z","iopub.status.idle":"2024-10-11T11:57:56.292818Z","shell.execute_reply":"2024-10-11T11:57:56.291617Z","shell.execute_reply.started":"2024-10-11T11:57:56.072576Z"},"id":"-Yan5vLDyj-3","trusted":true},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"0q1gRMAsyknz"},"source":["# Verification Task"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.294769Z","iopub.status.busy":"2024-10-11T11:57:56.294138Z","iopub.status.idle":"2024-10-11T11:57:56.304239Z","shell.execute_reply":"2024-10-11T11:57:56.303449Z","shell.execute_reply.started":"2024-10-11T11:57:56.294721Z"},"id":"SSGeDCi-wa1W","trusted":true},"outputs":[],"source":["def valid_epoch_ver(model, pair_data_loader, device, config):\n","\n","    model.eval()\n","    scores = []\n","    match_labels = []\n","    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n","    for i, (images1, images2, labels) in enumerate(pair_data_loader):\n","\n","        # match_labels = match_labels.to(device)\n","        images = torch.cat([images1, images2], dim=0).to(device)\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","\n","        feats = F.normalize(outputs['feats'], dim=1)\n","        feats1, feats2 = feats.chunk(2)\n","        similarity = F.cosine_similarity(feats1, feats2)\n","        scores.append(similarity.cpu().numpy())\n","        match_labels.append(labels.cpu().numpy())\n","        batch_bar.update()\n","\n","    scores = np.concatenate(scores)\n","    match_labels = np.concatenate(match_labels)\n","\n","    FPRs=['1e-4', '5e-4', '1e-3', '5e-3', '5e-2']\n","    metric_dict = get_ver_metrics(match_labels.tolist(), scores.tolist(), FPRs)\n","    print(metric_dict)\n","\n","    return metric_dict['ACC'], metric_dict['EER']\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"piblCbe5yotj"},"source":["# WandB"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T11:57:56.309385Z","iopub.status.busy":"2024-10-11T11:57:56.309078Z","iopub.status.idle":"2024-10-11T11:57:57.491710Z","shell.execute_reply":"2024-10-11T11:57:57.490948Z","shell.execute_reply.started":"2024-10-11T11:57:56.309355Z"},"id":"HTIkCXBQyoM0","trusted":true},"outputs":[],"source":["wandb.login(key=\"65ab01e861811be49c0d4f76312a0729eb26cdfc\") # API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"markdown","metadata":{},"source":["### For the other Runs I resumed the model with the best verifacation since it was corresponding to the lowest EER. The name  of my wandb runs include High-Resumerunning2:11-10-2024-SERSNET-Run3SGDFinal, High Cutoff Submission, Trial Run with dates and others.  For Wandb we used one account as team where we monitered the run and help each other. all my runs does not have my name on them compare to others. I resumed on notebook 2 times but its not what i selected which is why code for resuming is comment"]},{"cell_type":"markdown","metadata":{},"source":["### On epoch 50 I get the best EER and validation retrival\n","### Epoch 50/70:\n","## 499 Train Cls. Acc 78.3812%  Train Cls. Loss 1.2145  Learning Rate 0.0001\n","### 500 Val Cls. Acc 80.0565%  Val Cls. Loss 1.0916\n","### 501 {'ACC': 90.4, 'EER': 9.775967413441961, 'AUC': 96.0667256191006, 'TPRs': [('TPR@FPR=1e-4', 44.806517311608964), ('TPR@FPR=5e-4', 44.806517311608964), ('TPR@FPR=1e-3', 44.806517311608964), ####('TPR@FPR=5e-3', 47.45417515274949), ('TPR@FPR=5e-2', 82.68839103869654)]}\n","### 502 Val Ret. Acc 90.4000%\n","### c503 Saved best classification model with Val Cls. Acc: 80.0565%"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T12:19:41.345124Z","iopub.status.busy":"2024-10-11T12:19:41.344716Z","iopub.status.idle":"2024-10-11T12:19:44.954507Z","shell.execute_reply":"2024-10-11T12:19:44.953655Z","shell.execute_reply.started":"2024-10-11T12:19:41.345083Z"},"id":"GLNNqwV4ysNP","trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"12-10-2024 Trial run60 Epoch\", ## Wandb creates random run names if you skip this field\n","    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    #id = 'g4v1lxiq',### Insert specific run id here if you want to resume a previous run\n","    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account\n","    config = config ### Wandb Config for your run\n",")\n","# Restore the best retrieval model checkpoint\n","#Res_model = wandb.restore('checkpoint/best_ret.pth')\n","#checkpoint = torch.load(Res_model.name)\n","\n","# Load only the model state dict from the checkpoint\n","#model.load_state_dict(checkpoint[\"model_state_dict\"])"]},{"cell_type":"markdown","metadata":{"id":"t0RrtpFKzH3k"},"source":["# Checkpointing and Loading Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T12:19:49.768651Z","iopub.status.busy":"2024-10-11T12:19:49.768280Z","iopub.status.idle":"2024-10-11T12:19:49.777320Z","shell.execute_reply":"2024-10-11T12:19:49.776519Z","shell.execute_reply.started":"2024-10-11T12:19:49.768614Z"},"id":"dDFmC8hpzLOq","trusted":true},"outputs":[],"source":["# Uncomment the line for saving the scheduler save dict if you are using a scheduler\n","def save_model(model, optimizer, scheduler, metrics, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         'scheduler_state_dict'     : scheduler.state_dict(),\n","         'metric'                   : metrics,\n","         'epoch'                    : epoch},\n","         path)\n","\n","\n","def load_model(model, optimizer=None, scheduler=None, path='./checkpoint.pth'):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    if optimizer is not None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    else:\n","        optimizer = None\n","    if scheduler is not None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","    else:\n","        scheduler = None\n","    epoch = checkpoint['epoch']\n","    metrics = checkpoint['metric']\n","    return model, optimizer, scheduler, epoch, metrics"]},{"cell_type":"markdown","metadata":{"id":"wpFT7iriy5bi"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T12:19:51.319454Z","iopub.status.busy":"2024-10-11T12:19:51.318720Z","iopub.status.idle":"2024-10-11T12:19:51.324211Z","shell.execute_reply":"2024-10-11T12:19:51.323301Z","shell.execute_reply.started":"2024-10-11T12:19:51.319415Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T12:19:51.666103Z","iopub.status.busy":"2024-10-11T12:19:51.665803Z","iopub.status.idle":"2024-10-11T12:19:51.674820Z","shell.execute_reply":"2024-10-11T12:19:51.674011Z","shell.execute_reply.started":"2024-10-11T12:19:51.666071Z"},"id":"XUAa3m2h0eCD","trusted":true},"outputs":[],"source":["def test_epoch_ver(model, pair_data_loader, config):\n","\n","    model.eval()\n","    scores = []\n","    batch_bar = tqdm(total=len(pair_data_loader), dynamic_ncols=True, position=0, leave=False, desc='Val Veri.')\n","    for i, (images1, images2) in enumerate(pair_data_loader):\n","\n","        images = torch.cat([images1, images2], dim=0).to(DEVICE)\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","\n","        feats = F.normalize(outputs['feats'], dim=1)\n","        feats1, feats2 = feats.chunk(2)\n","        similarity = F.cosine_similarity(feats1, feats2)\n","        scores.extend(similarity.cpu().numpy().tolist())\n","        batch_bar.update()\n","\n","    return scores"]},{"cell_type":"markdown","metadata":{},"source":["### For the training loop, I included the code to moniter the EER on wandb chart and also be able to save the best verification model and its corresponding CSV file on wandb. \n","### For this run I initiated 70 epoch but my kaggle crashed when it was running the 63th epoch and since I added  the code to retrieve the csv of best model,\n","### I stopped there where I used RESSE Network, SGD as optimizer and CosineAnealing as scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-11T12:19:52.054621Z","iopub.status.busy":"2024-10-11T12:19:52.053787Z"},"id":"59FcCeJfy3Zm","trusted":true},"outputs":[],"source":["e = 0\n","best_valid_cls_acc = 0.0\n","eval_cls = True\n","best_valid_ret_acc = 0.0\n","for epoch in range(e, config['epochs']):\n","    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n","\n","    # Train\n","    train_cls_acc, train_loss = train_epoch(model, train_loader, optimizer, scheduler, scaler, DEVICE, config)\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","    print(\"\\nEpoch {}/{}: \\nTrain Cls. Acc {:.04f}%\\t Train Cls. Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n","        epoch + 1, config['epochs'], train_cls_acc, train_loss, curr_lr))\n","\n","    # Metrics for tracking\n","    metrics = {\n","        'train_cls_acc': train_cls_acc,\n","        'train_loss': train_loss,\n","    }\n","\n","    # Validation (classification)\n","    if eval_cls:\n","        valid_cls_acc, valid_loss = valid_epoch_cls(model, val_loader, DEVICE, config)\n","        print(\"Val Cls. Acc {:.04f}%\\t Val Cls. Loss {:.04f}\".format(valid_cls_acc, valid_loss))\n","        metrics.update({\n","            'valid_cls_acc': valid_cls_acc,\n","            'valid_loss': valid_loss,\n","        })\n","\n","    # Validation (retrieval)\n","    valid_ret_acc, valid_eer = valid_epoch_ver(model, pair_dataloader, DEVICE, config)\n","    print(\"Val Ret. Acc {:.04f}%\".format(valid_ret_acc))\n","    metrics.update({\n","        'valid_ret_acc': valid_ret_acc,\n","        'valid_eer': valid_eer\n","    })\n","    \n","\n","    # Save the best classification model only\n","    if eval_cls and valid_cls_acc >= best_valid_cls_acc:\n","        best_valid_cls_acc = valid_cls_acc\n","        best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n","        torch.save(model.state_dict(), best_model_path)\n","        wandb.save(best_model_path, base_path=config['checkpoint_dir'])  # Ensure the correct base path is used\n","        \n","        print(f\"Saved best classification model with Val Cls. Acc: {valid_cls_acc:.04f}%\")\n","    if valid_ret_acc >= best_valid_ret_acc:\n","        best_valid_ret_acc = valid_ret_acc\n","        save_model(model, optimizer, scheduler, metrics, epoch, os.path.join(config['checkpoint_dir'], 'best_ret_final.pth'))\n","        wandb.save(os.path.join(config['checkpoint_dir'], 'best_ret_final.pth'))\n","        print(\"Saved best retrieval model\")        \n","        scores = test_epoch_ver(model, test_pair_dataloader, config)\n","        csv_path = \"Final_submission_best_ret_epoch_.csv\"\n","        with open(csv_path, \"w+\") as f:\n","            f.write(\"ID,Label\\n\")\n","            for i in range(len(scores)):\n","                f.write(\"{},{}\\n\".format(i, scores[i]))\n","        wandb.save(csv_path)\n","        print(\"Saved CSV for best retrieval model at epoch \")\n","        \n","    \n","\n","    # Log metrics to WandB\n","    if run is not None:\n","        run.log(metrics)\n","\n","    # Learning rate scheduler step\n","    if scheduler is not None:\n","        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n","            scheduler.step(valid_loss)\n","        else:\n","            scheduler.step()\n","\n","print('Completed!')\n"]},{"cell_type":"markdown","metadata":{"id":"7aZ7yRdBKraO"},"source":["# Testing and Kaggle Submission (Verification)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-11T12:19:23.120749Z","iopub.status.idle":"2024-10-11T12:19:23.121104Z","shell.execute_reply":"2024-10-11T12:19:23.120954Z","shell.execute_reply.started":"2024-10-11T12:19:23.120936Z"},"id":"KZob_EOvIb65","outputId":"aec87391-d825-4eb4-f439-7a97681dbd39","trusted":true},"outputs":[],"source":["scores = test_epoch_ver(model, test_pair_dataloader, config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-11T12:19:23.122277Z","iopub.status.idle":"2024-10-11T12:19:23.122597Z","shell.execute_reply":"2024-10-11T12:19:23.122451Z","shell.execute_reply.started":"2024-10-11T12:19:23.122434Z"},"id":"ETSImix3AYy_","trusted":true},"outputs":[],"source":["with open(\"Final_submission_1.csv\", \"w+\") as f:\n","    f.write(\"ID,Label\\n\")\n","    for i in range(len(scores)):\n","        f.write(\"{},{}\\n\".format(i, scores[i]))"]},{"cell_type":"markdown","metadata":{},"source":["Saving the CSV or prediction in the output of my kaggle for later use."]},{"cell_type":"markdown","metadata":{},"source":["## Here is the Link to Wandb study team :\n","# https://api.wandb.ai/links/angeizabayo05-carnegie-mellon-university/wbhievnv\n","### For this homework we tested and experimented different technique in order to achieve the required output and as shown in the link we did many runs and the one yield or returned the high cutoff score is \"12-10-2024 Trial run60 Epoch\""]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
